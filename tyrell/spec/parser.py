# The file was automatically generated by Lark v0.6.7
#
#
#   Lark Stand-alone Generator Tool
# ----------------------------------
# Generates a stand-alone LALR(1) parser with a standard lexer
#
# Git:    https://github.com/erezsh/lark
# Author: Erez Shinan (erezshin@gmail.com)
#
#
#    >>> LICENSE
#
#    This tool and its generated code use a separate license from Lark.
#
#    It is licensed under GPLv2 or above.
#
#    If you wish to purchase a commercial license for this tool and its
#    generated code, contact me via email.
#
#    If GPL is incompatible with your free or open-source project,
#    contact me and we'll work it out (for free).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    See <http://www.gnu.org/licenses/>.
#
#

class LarkError(Exception):
    pass

class GrammarError(LarkError):
    pass

class ParseError(LarkError):
    pass

class LexError(LarkError):
    pass

class UnexpectedInput(LarkError):
    pos_in_stream = None

    def get_context(self, text, span=40):
        pos = self.pos_in_stream
        start = max(pos - span, 0)
        end = pos + span
        before = text[start:pos].rsplit('\n', 1)[-1]
        after = text[pos:end].split('\n', 1)[0]
        return before + after + '\n' + ' ' * len(before) + '^\n'

    def match_examples(self, parse_fn, examples):
        """ Given a parser instance and a dictionary mapping some label with
            some malformed syntax examples, it'll return the label for the
            example that bests matches the current error.
        """
        assert self.state is not None, "Not supported for this exception"

        candidate = None
        for label, example in examples.items():
            assert not isinstance(example, STRING_TYPE)

            for malformed in example:
                try:
                    parse_fn(malformed)
                except UnexpectedInput as ut:
                    if ut.state == self.state:
                        try:
                            if ut.token == self.token:  # Try exact match first
                                return label
                        except AttributeError:
                            pass
                        if not candidate:
                            candidate = label

        return candidate


class UnexpectedCharacters(LexError, UnexpectedInput):
    def __init__(self, seq, lex_pos, line, column, allowed=None, considered_tokens=None, state=None):
        message = "No terminal defined for '%s' at line %d col %d" % (seq[lex_pos], line, column)

        self.line = line
        self.column = column
        self.allowed = allowed
        self.considered_tokens = considered_tokens
        self.pos_in_stream = lex_pos
        self.state = state

        message += '\n\n' + self.get_context(seq)
        if allowed:
            message += '\nExpecting: %s\n' % allowed

        super(UnexpectedCharacters, self).__init__(message)



class UnexpectedToken(ParseError, UnexpectedInput):
    def __init__(self, token, expected, considered_rules=None, state=None):
        self.token = token
        self.expected = expected     # XXX str shouldn't necessary
        self.line = getattr(token, 'line', '?')
        self.column = getattr(token, 'column', '?')
        self.considered_rules = considered_rules
        self.state = state
        self.pos_in_stream = getattr(token, 'pos_in_stream', None)

        message = ("Unexpected token %r at line %s, column %s.\n"
                   "Expected one of: \n\t* %s\n"
                   % (token, self.line, self.column, '\n\t* '.join(self.expected)))

        super(UnexpectedToken, self).__init__(message)

class VisitError(LarkError):
    def __init__(self, tree, orig_exc):
        self.tree = tree
        self.orig_exc = orig_exc

        message = 'Error trying to process rule "%s":\n\n%s' % (tree.data, orig_exc)
        super(VisitError, self).__init__(message)

try:
    STRING_TYPE = basestring
except NameError:   # Python 3
    STRING_TYPE = str


import types
from functools import wraps, partial
from contextlib import contextmanager

Str = type(u'')
try:
    classtype = types.ClassType # Python2
except AttributeError:
    classtype = type    # Python3

def smart_decorator(f, create_decorator):
    if isinstance(f, types.FunctionType):
        return wraps(f)(create_decorator(f, True))

    elif isinstance(f, (classtype, type, types.BuiltinFunctionType)):
        return wraps(f)(create_decorator(f, False))

    elif isinstance(f, types.MethodType):
        return wraps(f)(create_decorator(f.__func__, True))

    elif isinstance(f, partial):
        # wraps does not work for partials in 2.7: https://bugs.python.org/issue3445
        return create_decorator(f.__func__, True)

    else:
        return create_decorator(f.__func__.__call__, True)



class Meta:
    pass

class Tree(object):
    def __init__(self, data, children, meta=None):
        self.data = data
        self.children = children
        self._meta = meta

    @property
    def meta(self):
        if self._meta is None:
            self._meta = Meta()
        return self._meta

    def __repr__(self):
        return 'Tree(%s, %s)' % (self.data, self.children)

    def _pretty_label(self):
        return self.data

    def _pretty(self, level, indent_str):
        if len(self.children) == 1 and not isinstance(self.children[0], Tree):
            return [ indent_str*level, self._pretty_label(), '\t', '%s' % (self.children[0],), '\n']

        l = [ indent_str*level, self._pretty_label(), '\n' ]
        for n in self.children:
            if isinstance(n, Tree):
                l += n._pretty(level+1, indent_str)
            else:
                l += [ indent_str*(level+1), '%s' % (n,), '\n' ]

        return l

    def pretty(self, indent_str='  '):
        return ''.join(self._pretty(0, indent_str))

    def __eq__(self, other):
        try:
            return self.data == other.data and self.children == other.children
        except AttributeError:
            return False

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash((self.data, tuple(self.children)))

from inspect import getmembers, getmro

class Discard(Exception):
    pass

# Transformers

class Transformer:
    """Visits the tree recursively, starting with the leaves and finally the root (bottom-up)

    Calls its methods (provided by user via inheritance) according to tree.data
    The returned value replaces the old one in the structure.

    Can be used to implement map or reduce.
    """

    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            try:
                if getattr(f, 'meta', False):
                    return f(children, tree.meta)
                elif getattr(f, 'inline', False):
                    return f(*children)
                elif getattr(f, 'whole_tree', False):
                    if new_children is not None:
                        raise NotImplementedError("Doesn't work with the base Transformer class")
                    return f(tree)
                else:
                    return f(children)
            except (GrammarError, Discard):
                raise
            except Exception as e:
                raise VisitError(tree, e)

    def _transform_children(self, children):
        for c in children:
            try:
                yield self._transform_tree(c) if isinstance(c, Tree) else c
            except Discard:
                pass

    def _transform_tree(self, tree):
        children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree, children)

    def transform(self, tree):
        return self._transform_tree(tree)

    def __mul__(self, other):
        return TransformerChain(self, other)

    def __default__(self, data, children, meta):
        "Default operation on tree (for override)"
        return Tree(data, children, meta)

    @classmethod
    def _apply_decorator(cls, decorator, **kwargs):
        mro = getmro(cls)
        assert mro[0] is cls
        libmembers = {name for _cls in mro[1:] for name, _ in getmembers(_cls)}
        for name, value in getmembers(cls):
            if name.startswith('_') or name in libmembers:
                continue

            # Skip if v_args already applied (at the function level)
            if hasattr(cls.__dict__[name], 'vargs_applied'):
                continue

            static = isinstance(cls.__dict__[name], (staticmethod, classmethod))
            setattr(cls, name, decorator(value, static=static, **kwargs))
        return cls


class InlineTransformer(Transformer):   # XXX Deprecated
    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            return f(*children)


class TransformerChain(object):
    def __init__(self, *transformers):
        self.transformers = transformers

    def transform(self, tree):
        for t in self.transformers:
            tree = t.transform(tree)
        return tree

    def __mul__(self, other):
        return TransformerChain(*self.transformers + (other,))


class Transformer_InPlace(Transformer):
    "Non-recursive. Changes the tree in-place instead of returning new instances"
    def _transform_tree(self, tree):           # Cancel recursion
        return self._call_userfunc(tree)

    def transform(self, tree):
        for subtree in tree.iter_subtrees():
            subtree.children = list(self._transform_children(subtree.children))

        return self._transform_tree(tree)


class Transformer_InPlaceRecursive(Transformer):
    "Recursive. Changes the tree in-place instead of returning new instances"
    def _transform_tree(self, tree):
        tree.children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree)



# Visitors

class VisitorBase:
    def _call_userfunc(self, tree):
        return getattr(self, tree.data, self.__default__)(tree)

    def __default__(self, tree):
        "Default operation on tree (for override)"
        return tree


class Visitor(VisitorBase):
    """Bottom-up visitor, non-recursive

    Visits the tree, starting with the leaves and finally the root (bottom-up)
    Calls its methods (provided by user via inheritance) according to tree.data
    """


    def visit(self, tree):
        for subtree in tree.iter_subtrees():
            self._call_userfunc(subtree)
        return tree

class Visitor_Recursive(VisitorBase):
    """Bottom-up visitor, recursive

    Visits the tree, starting with the leaves and finally the root (bottom-up)
    Calls its methods (provided by user via inheritance) according to tree.data
    """

    def visit(self, tree):
        for child in tree.children:
            if isinstance(child, Tree):
                self.visit(child)

        f = getattr(self, tree.data, self.__default__)
        f(tree)
        return tree



def visit_children_decor(func):
    "See Interpreter"
    @wraps(func)
    def inner(cls, tree):
        values = cls.visit_children(tree)
        return func(cls, values)
    return inner


class Interpreter:
    """Top-down visitor, recursive

    Visits the tree, starting with the root and finally the leaves (top-down)
    Calls its methods (provided by user via inheritance) according to tree.data

    Unlike Transformer and Visitor, the Interpreter doesn't automatically visit its sub-branches.
    The user has to explicitly call visit_children, or use the @visit_children_decor
    """
    def visit(self, tree):
        return getattr(self, tree.data)(tree)

    def visit_children(self, tree):
        return [self.visit(child) if isinstance(child, Tree) else child
                for child in tree.children]

    def __getattr__(self, name):
        return self.__default__

    def __default__(self, tree):
        return self.visit_children(tree)




# Decorators

def _apply_decorator(obj, decorator, **kwargs):
    try:
        _apply = obj._apply_decorator
    except AttributeError:
        return decorator(obj, **kwargs)
    else:
        return _apply(decorator, **kwargs)



def _inline_args__func(func):
    @wraps(func)
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, children):
                return _f(self, *children)
        else:
            def f(self, children):
                return _f(*children)
        return f

    return smart_decorator(func, create_decorator)


def inline_args(obj):   # XXX Deprecated
    return _apply_decorator(obj, _inline_args__func)



def _visitor_args_func_dec(func, inline=False, meta=False, whole_tree=False, static=False):
    assert [whole_tree, meta, inline].count(True) <= 1
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, *args, **kwargs):
                return _f(self, *args, **kwargs)
        else:
            def f(self, *args, **kwargs):
                return _f(*args, **kwargs)
        return f

    if static:
        f = wraps(func)(create_decorator(func, False))
    else:
        f = smart_decorator(func, create_decorator)
    f.vargs_applied = True
    f.inline = inline
    f.meta = meta
    f.whole_tree = whole_tree
    return f

def v_args(inline=False, meta=False, tree=False):
    "A convenience decorator factory, for modifying the behavior of user-supplied visitor methods"
    if [tree, meta, inline].count(True) > 1:
        raise ValueError("Visitor functions can either accept tree, or meta, or be inlined. These cannot be combined.")
    def _visitor_args_dec(obj):
        return _apply_decorator(obj, _visitor_args_func_dec, inline=inline, meta=meta, whole_tree=tree)
    return _visitor_args_dec



class Indenter:
    def __init__(self):
        self.paren_level = None
        self.indent_level = None
        assert self.tab_len > 0

    def handle_NL(self, token):
        if self.paren_level > 0:
            return

        yield token

        indent_str = token.rsplit('\n', 1)[1] # Tabs and spaces
        indent = indent_str.count(' ') + indent_str.count('\t') * self.tab_len

        if indent > self.indent_level[-1]:
            self.indent_level.append(indent)
            yield Token.new_borrow_pos(self.INDENT_type, indent_str, token)
        else:
            while indent < self.indent_level[-1]:
                self.indent_level.pop()
                yield Token.new_borrow_pos(self.DEDENT_type, indent_str, token)

            assert indent == self.indent_level[-1], '%s != %s' % (indent, self.indent_level[-1])

    def _process(self, stream):
        for token in stream:
            if token.type == self.NL_type:
                for t in self.handle_NL(token):
                    yield t
            else:
                yield token

            if token.type in self.OPEN_PAREN_types:
                self.paren_level += 1
            elif token.type in self.CLOSE_PAREN_types:
                self.paren_level -= 1
                assert self.paren_level >= 0

        while len(self.indent_level) > 1:
            self.indent_level.pop()
            yield Token(self.DEDENT_type, '')

        assert self.indent_level == [0], self.indent_level

    def process(self, stream):
        self.paren_level = 0
        self.indent_level = [0]
        return self._process(stream)

    # XXX Hack for ContextualLexer. Maybe there's a more elegant solution?
    @property
    def always_accept(self):
        return (self.NL_type,)


class Token(Str):
    __slots__ = ('type', 'pos_in_stream', 'value', 'line', 'column', 'end_line', 'end_column')

    def __new__(cls, type_, value, pos_in_stream=None, line=None, column=None):
        self = super(Token, cls).__new__(cls, value)
        self.type = type_
        self.pos_in_stream = pos_in_stream
        self.value = value
        self.line = line
        self.column = column
        self.end_line = None
        self.end_column = None
        return self

    @classmethod
    def new_borrow_pos(cls, type_, value, borrow_t):
        return cls(type_, value, borrow_t.pos_in_stream, line=borrow_t.line, column=borrow_t.column)

    def __reduce__(self):
        return (self.__class__, (self.type, self.value, self.pos_in_stream, self.line, self.column, ))

    def __repr__(self):
        return 'Token(%s, %r)' % (self.type, self.value)

    def __deepcopy__(self, memo):
        return Token(self.type, self.value, self.pos_in_stream, self.line, self.column)

    def __eq__(self, other):
        if isinstance(other, Token) and self.type != other.type:
            return False

        return Str.__eq__(self, other)

    __hash__ = Str.__hash__


class LineCounter:
    def __init__(self):
        self.newline_char = '\n'
        self.char_pos = 0
        self.line = 1
        self.column = 1
        self.line_start_pos = 0

    def feed(self, token, test_newline=True):
        """Consume a token and calculate the new line & column.

        As an optional optimization, set test_newline=False is token doesn't contain a newline.
        """
        if test_newline:
            newlines = token.count(self.newline_char)
            if newlines:
                self.line += newlines
                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1

        self.char_pos += len(token)
        self.column = self.char_pos - self.line_start_pos + 1

class _Lex:
    "Built to serve both Lexer and ContextualLexer"
    def __init__(self, lexer, state=None):
        self.lexer = lexer
        self.state = state

    def lex(self, stream, newline_types, ignore_types):
        newline_types = frozenset(newline_types)
        ignore_types = frozenset(ignore_types)
        line_ctr = LineCounter()

        while line_ctr.char_pos < len(stream):
            lexer = self.lexer
            for mre, type_from_index in lexer.mres:
                m = mre.match(stream, line_ctr.char_pos)
                if not m:
                    continue

                t = None
                value = m.group(0)
                type_ = type_from_index[m.lastindex]
                if type_ not in ignore_types:
                    t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                    if t.type in lexer.callback:
                        t = lexer.callback[t.type](t)
                        if not isinstance(t, Token):
                            raise ValueError("Callbacks must return a token (returned %r)" % t)
                    yield t
                else:
                    if type_ in lexer.callback:
                        t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                        lexer.callback[type_](t)

                line_ctr.feed(value, type_ in newline_types)
                if t:
                    t.end_line = line_ctr.line
                    t.end_column = line_ctr.column

                break
            else:
                raise UnexpectedCharacters(stream, line_ctr.char_pos, line_ctr.line, line_ctr.column, state=self.state)


class UnlessCallback:
    def __init__(self, mres):
        self.mres = mres

    def __call__(self, t):
        for mre, type_from_index in self.mres:
            m = mre.match(t.value)
            if m:
                t.type = type_from_index[m.lastindex]
                break
        return t

class CallChain:
    def __init__(self, callback1, callback2, cond):
        self.callback1 = callback1
        self.callback2 = callback2
        self.cond = cond

    def __call__(self, t):
        t2 = self.callback1(t)
        return self.callback2(t) if self.cond(t2) else t2



from functools import partial, wraps


class ExpandSingleChild:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        if len(children) == 1:
            return children[0]
        else:
            return self.node_builder(children)

class PropagatePositions:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        res = self.node_builder(children)

        if isinstance(res, Tree) and getattr(res.meta, 'empty', True):
            res.meta.empty = True

            for c in children:
                if isinstance(c, Tree) and c.children and not c.meta.empty:
                    res.meta.line = c.meta.line
                    res.meta.column = c.meta.column
                    res.meta.start_pos = c.meta.start_pos
                    res.meta.empty = False
                    break
                elif isinstance(c, Token):
                    res.meta.line = c.line
                    res.meta.column = c.column
                    res.meta.start_pos = c.pos_in_stream
                    res.meta.empty = False
                    break

            for c in reversed(children):
                if isinstance(c, Tree) and c.children and not c.meta.empty:
                    res.meta.end_line = c.meta.end_line
                    res.meta.end_column = c.meta.end_column
                    res.meta.end_pos = c.meta.end_pos
                    res.meta.empty = False
                    break
                elif isinstance(c, Token):
                    res.meta.end_line = c.end_line
                    res.meta.end_column = c.end_column
                    res.meta.end_pos = c.pos_in_stream + len(c.value)
                    res.meta.empty = False
                    break

        return res


class ChildFilter:
    def __init__(self, to_include, append_none, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include
        self.append_none = append_none

    def __call__(self, children):
        filtered = []

        for i, to_expand, add_none in self.to_include:
            if add_none:
                filtered += [None] * add_none
            if to_expand:
                filtered += children[i].children
            else:
                filtered.append(children[i])

        if self.append_none:
            filtered += [None] * self.append_none

        return self.node_builder(filtered)

class ChildFilterLALR(ChildFilter):
    "Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"

    def __call__(self, children):
        filtered = []
        for i, to_expand, add_none in self.to_include:
            if add_none:
                filtered += [None] * add_none
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   # Optimize for left-recursion
                    filtered = children[i].children
            else:
                filtered.append(children[i])

        if self.append_none:
            filtered += [None] * self.append_none

        return self.node_builder(filtered)

class ChildFilterLALR_NoPlaceholders(ChildFilter):
    "Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"
    def __init__(self, to_include, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   # Optimize for left-recursion
                    filtered = children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

def _should_expand(sym):
    return not sym.is_term and sym.name.startswith('_')

def maybe_create_child_filter(expansion, keep_all_tokens, ambiguous, _empty_indices):
    # Prepare empty_indices as: How many Nones to insert at each index?
    if _empty_indices:
        assert _empty_indices.count(False) == len(expansion)
        s = ''.join(str(int(b)) for b in _empty_indices)
        empty_indices = [len(ones) for ones in s.split('0')]
        assert len(empty_indices) == len(expansion)+1, (empty_indices, len(expansion))
    else:
        empty_indices = [0] * (len(expansion)+1)

    to_include = []
    nones_to_add = 0
    for i, sym in enumerate(expansion):
        nones_to_add += empty_indices[i]
        if keep_all_tokens or not (sym.is_term and sym.filter_out):
            to_include.append((i, _should_expand(sym), nones_to_add))
            nones_to_add = 0

    nones_to_add += empty_indices[len(expansion)]

    if _empty_indices or len(to_include) < len(expansion) or any(to_expand for i, to_expand,_ in to_include):
        if _empty_indices or ambiguous:
            return partial(ChildFilter if ambiguous else ChildFilterLALR, to_include, nones_to_add)
        else:
            # LALR without placeholders
            return partial(ChildFilterLALR_NoPlaceholders, [(i, x) for i,x,_ in to_include])


class Callback(object):
    pass


def ptb_inline_args(func):
    @wraps(func)
    def f(children):
        return func(*children)
    return f



class ParseTreeBuilder:
    def __init__(self, rules, tree_class, propagate_positions=False, keep_all_tokens=False, ambiguous=False, maybe_placeholders=False):
        self.tree_class = tree_class
        self.propagate_positions = propagate_positions
        self.always_keep_all_tokens = keep_all_tokens
        self.ambiguous = ambiguous
        self.maybe_placeholders = maybe_placeholders

        self.rule_builders = list(self._init_builders(rules))

        self.user_aliases = {}

    def _init_builders(self, rules):
        for rule in rules:
            options = rule.options
            keep_all_tokens = self.always_keep_all_tokens or (options.keep_all_tokens if options else False)
            expand_single_child = options.expand1 if options else False

            wrapper_chain = filter(None, [
                (expand_single_child and not rule.alias) and ExpandSingleChild,
                maybe_create_child_filter(rule.expansion, keep_all_tokens, self.ambiguous, options.empty_indices if self.maybe_placeholders and options else None),
                self.propagate_positions and PropagatePositions,
            ])

            yield rule, wrapper_chain


    def create_callback(self, transformer=None):
        callback = Callback()

        i = 0
        for rule, wrapper_chain in self.rule_builders:
            internal_callback_name = '_cb%d_%s' % (i, rule.origin)
            i += 1

            user_callback_name = rule.alias or rule.origin.name
            try:
                f = getattr(transformer, user_callback_name)
                assert not getattr(f, 'meta', False), "Meta args not supported for internal transformer"
                # XXX InlineTransformer is deprecated!
                if getattr(f, 'inline', False) or isinstance(transformer, InlineTransformer):
                    f = ptb_inline_args(f)
            except AttributeError:
                f = partial(self.tree_class, user_callback_name)

            self.user_aliases[rule] = rule.alias
            rule.alias = internal_callback_name

            for w in wrapper_chain:
                f = w(f)

            if hasattr(callback, internal_callback_name):
                raise GrammarError("Rule '%s' already exists" % (rule,))
            setattr(callback, internal_callback_name, f)

        return callback



class _Parser:
    def __init__(self, parse_table, callbacks):
        self.states = parse_table.states
        self.start_state = parse_table.start_state
        self.end_state = parse_table.end_state
        self.callbacks = callbacks

    def parse(self, seq, set_state=None):
        token = None
        stream = iter(seq)
        states = self.states

        state_stack = [self.start_state]
        value_stack = []

        if set_state: set_state(self.start_state)

        def get_action(token):
            state = state_stack[-1]
            try:
                return states[state][token.type]
            except KeyError:
                expected = [s for s in states[state].keys() if s.isupper()]
                raise UnexpectedToken(token, expected, state=state)

        def reduce(rule):
            size = len(rule.expansion)
            if size:
                s = value_stack[-size:]
                del state_stack[-size:]
                del value_stack[-size:]
            else:
                s = []

            value = self.callbacks[rule](s)

            _action, new_state = states[state_stack[-1]][rule.origin.name]
            assert _action is Shift
            state_stack.append(new_state)
            value_stack.append(value)

        # Main LALR-parser loop
        for token in stream:
            while True:
                action, arg = get_action(token)
                assert arg != self.end_state

                if action is Shift:
                    state_stack.append(arg)
                    value_stack.append(token)
                    if set_state: set_state(arg)
                    break # next token
                else:
                    reduce(arg)

        token = Token.new_borrow_pos('$END', '', token) if token else Token('$END', '', 0, 1, 1)
        while True:
            _action, arg = get_action(token)
            if _action is Shift:
                assert arg == self.end_state
                val ,= value_stack
                return val
            else:
                reduce(arg)


class Symbol(object):
    is_term = NotImplemented

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        assert isinstance(other, Symbol), other
        return self.is_term == other.is_term and self.name == other.name

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash(self.name)

    def __repr__(self):
        return '%s(%r)' % (type(self).__name__, self.name)

    fullrepr = property(__repr__)

class Terminal(Symbol):
    is_term = True

    def __init__(self, name, filter_out=False):
        self.name = name
        self.filter_out = filter_out

    @property
    def fullrepr(self):
        return '%s(%r, %r)' % (type(self).__name__, self.name, self.filter_out)


class NonTerminal(Symbol):
    is_term = False

class Rule(object):
    """
        origin : a symbol
        expansion : a list of symbols
    """
    def __init__(self, origin, expansion, alias=None, options=None):
        self.origin = origin
        self.expansion = expansion
        self.alias = alias
        self.options = options

    def __str__(self):
        return '<%s : %s>' % (self.origin.name, ' '.join(x.name for x in self.expansion))

    def __repr__(self):
        return 'Rule(%r, %r, %r, %r)' % (self.origin, self.expansion, self.alias, self.options)

    def __hash__(self):
        return hash((self.origin, tuple(self.expansion)))
    def __eq__(self, other):
        if not isinstance(other, Rule):
            return False
        return self.origin == other.origin and self.expansion == other.expansion


class RuleOptions:
    def __init__(self, keep_all_tokens=False, expand1=False, priority=None):
        self.keep_all_tokens = keep_all_tokens
        self.expand1 = expand1
        self.priority = priority
        self.empty_indices = ()

    def __repr__(self):
        return 'RuleOptions(%r, %r, %r)' % (
            self.keep_all_tokens,
            self.expand1,
            self.priority,
        )

Shift = 0
Reduce = 1
import re
class LexerRegexps: pass
NEWLINE_TYPES = ['COMMENT', 'STRLIT', 'WS']
IGNORE_TYPES = ['WS', 'COMMENT']
LEXERS = {}
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[0] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[1] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[2] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PROGRAM>program)',
  {1: 'COMMENT', 3: 'WS', 4: 'PROGRAM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[3] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[4] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[5] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[6] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[7] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[8] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[9] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[10] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[11] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[12] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[13] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[14] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LSQB>\\[)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: '__ANON_0',
   6: 'COMMA',
   7: 'LBRACE',
   8: 'LSQB',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[15] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LSQB>\\[)',
  {1: 'COMMENT', 3: 'WS', 4: 'LSQB'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[16] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[17] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[18] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON', 5: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[19] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[20] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[21] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[22] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[23] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[24] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[25] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[26] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[27] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)',
  {1: 'COMMENT', 3: 'WS', 4: 'INT'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[28] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[29] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[30] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[31] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[32] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[33] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[34] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[35] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[36] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[37] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[38] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[39] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[40] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[41] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RSQB>\\])',
  {1: 'COMMENT', 3: 'WS', 4: 'RSQB'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[42] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[43] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[44] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[45] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[46] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[47] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[48] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[49] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[50] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[51] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<BOOL>bool)|(?P<REAL>real)|(?P<__ANON_8>int)',
  {1: 'COMMENT', 3: 'WS', 4: 'BOOL', 5: 'REAL', 6: '__ANON_8'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[52] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[53] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[54] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[55] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[56] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[57] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[58] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[59] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[60] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[61] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[62] = (lexer_regexps)
MRES = (
[('(?P<SNUMBER>(?:(?:\\+|\\-))?(?:(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?)|(?:[0-9])+))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'SNUMBER', 2: 'NAME', 3: 'COMMENT', 5: 'STRLIT', 6: 'WS', 7: 'RPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[63] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[64] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[65] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: '__ANON_0',
   6: 'COMMA',
   7: 'LBRACE',
   8: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[66] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[67] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[68] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[69] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[70] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[71] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[72] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[73] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[74] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[75] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[76] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[77] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[78] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[79] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_0>\\->)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_0',
   8: '__ANON_2',
   9: '__ANON_3',
   10: '__ANON_4',
   11: '__ANON_5',
   12: '__ANON_6',
   13: '__ANON_7',
   14: 'COMMA',
   15: 'LBRACE',
   16: 'LESSTHAN',
   17: 'MINUS',
   18: 'MORETHAN',
   19: 'PERCENT',
   20: 'PLUS',
   21: 'RPAR',
   22: 'SEMICOLON',
   23: 'SLASH',
   24: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[80] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[81] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[82] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[83] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[84] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[85] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[86] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[87] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: '__ANON_0',
   5: 'COMMA',
   6: 'LBRACE',
   7: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[88] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[89] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[90] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[91] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[92] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[93] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[94] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[95] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[96] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[97] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[98] = (lexer_regexps)
MRES = (
[('(?P<SNUMBER>(?:(?:\\+|\\-))?(?:(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?)|(?:[0-9])+))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'SNUMBER', 2: 'NAME', 3: 'COMMENT', 5: 'STRLIT', 6: 'WS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[99] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[100] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[101] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PLUS',
   17: 'RPAR',
   18: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[102] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[103] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MORETHAN>>)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MORETHAN',
   15: 'RPAR',
   16: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[104] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[105] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: 'RPAR',
   9: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[106] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_0>\\->)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<COLON>:)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LESSTHAN><)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_0',
   8: '__ANON_2',
   9: '__ANON_3',
   10: '__ANON_4',
   11: '__ANON_5',
   12: '__ANON_6',
   13: '__ANON_7',
   14: 'COLON',
   15: 'COMMA',
   16: 'LBRACE',
   17: 'LESSTHAN',
   18: 'LPAR',
   19: 'MINUS',
   20: 'MORETHAN',
   21: 'PERCENT',
   22: 'PLUS',
   23: 'RPAR',
   24: 'SEMICOLON',
   25: 'SLASH',
   26: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[107] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[108] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'FLOAT', 2: 'NAME', 3: 'COMMENT', 5: 'WS', 6: 'INT', 7: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[109] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[110] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[111] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[112] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[113] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'FLOAT', 2: 'NAME', 3: 'COMMENT', 5: 'WS', 6: 'INT', 7: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[114] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[115] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'FLOAT', 2: 'NAME', 3: 'COMMENT', 5: 'WS', 6: 'INT', 7: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[116] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[117] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[118] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[119] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[120] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS',
   10: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[121] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[122] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[123] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[124] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[125] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[126] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[127] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[128] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[129] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[130] = (lexer_regexps)
MRES = (
[('(?P<SNUMBER>(?:(?:\\+|\\-))?(?:(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?)|(?:[0-9])+))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'SNUMBER', 2: 'NAME', 3: 'COMMENT', 5: 'STRLIT', 6: 'WS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[131] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[132] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[133] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[134] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[135] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[136] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[137] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[138] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[139] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[140] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[141] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[142] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[143] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[144] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[145] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[146] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[147] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[148] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[149] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[150] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[151] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[152] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<THEN>then)',
  {1: 'COMMENT', 3: 'WS', 4: 'THEN'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[153] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS',
   10: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[154] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS',
   10: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[155] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS',
   10: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[156] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[157] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[158] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[159] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[160] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[161] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[162] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PLUS',
   17: 'RPAR',
   18: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[163] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'ELSE', 5: 'THEN', 6: 'RPAR', 7: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[164] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[165] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[166] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[167] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[168] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS',
   10: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[169] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MORETHAN>>)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MORETHAN',
   15: 'RPAR',
   16: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[170] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[171] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[172] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[173] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)',
  {1: 'COMMENT', 3: 'WS', 4: 'ELSE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[174] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[175] = (lexer_regexps)
MRES = (
[('(?P<FLOAT>(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'FLOAT',
   2: 'NAME',
   3: 'COMMENT',
   5: 'WS',
   6: 'INT',
   7: 'BANG',
   8: 'LPAR',
   9: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[176] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[177] = (lexer_regexps)
class ContextualLexer:
    def __init__(self):
        self.lexers = LEXERS
        self.set_parser_state(None)
    def set_parser_state(self, state):
        self.parser_state = state
    def lex(self, stream):
        newline_types = NEWLINE_TYPES
        ignore_types = IGNORE_TYPES
        lexers = LEXERS
        l = _Lex(lexers[self.parser_state], self.parser_state)
        for x in l.lex(stream, newline_types, ignore_types):
            yield x
            l.lexer = lexers[self.parser_state]
            l.state = self.parser_state
CON_LEXER = ContextualLexer()
def lex(stream):
    return CON_LEXER.lex(stream)
RULES = {
  0: Rule(NonTerminal('start'), [NonTerminal('type_decls'), NonTerminal('program_decl'), NonTerminal('func_decls'), NonTerminal('global_preds')], None, RuleOptions(False, False, None)),
  1: Rule(NonTerminal('type_decls'), [], None, RuleOptions(False, False, None)),
  2: Rule(NonTerminal('type_decls'), [NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  3: Rule(NonTerminal('type_decl'), [NonTerminal('enum_set_decl')], None, RuleOptions(False, True, None)),
  4: Rule(NonTerminal('type_decl'), [NonTerminal('value_decl')], None, RuleOptions(False, True, None)),
  5: Rule(NonTerminal('type_decl'), [NonTerminal('enum_decl')], None, RuleOptions(False, True, None)),
  6: Rule(NonTerminal('enum_decl'), [Terminal('ENUM', True), NonTerminal('type_name'), NonTerminal('enum_body')], None, RuleOptions(False, False, None)),
  7: Rule(NonTerminal('enum_set_decl'), [Terminal('ENUMSET', True), NonTerminal('type_name'), Terminal('LSQB', True), Terminal('INT', False), Terminal('RSQB', True), NonTerminal('enum_body')], None, RuleOptions(False, False, None)),
  8: Rule(NonTerminal('enum_body'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  9: Rule(NonTerminal('enum_body'), [Terminal('LBRACE', True), NonTerminal('enum_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  10: Rule(NonTerminal('enum_items'), [NonTerminal('enum_item')], None, RuleOptions(False, False, None)),
  11: Rule(NonTerminal('enum_items'), [NonTerminal('enum_item'), NonTerminal('__anon_star_1')], None, RuleOptions(False, False, None)),
  12: Rule(NonTerminal('value_decl'), [Terminal('VALUE', True), NonTerminal('type_name'), NonTerminal('value_body')], None, RuleOptions(False, False, None)),
  13: Rule(NonTerminal('value_body'), [Terminal('LBRACE', True), NonTerminal('value_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  14: Rule(NonTerminal('value_body'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  15: Rule(NonTerminal('value_items'), [NonTerminal('value_item'), NonTerminal('__anon_star_2')], None, RuleOptions(False, False, None)),
  16: Rule(NonTerminal('value_items'), [NonTerminal('value_item')], None, RuleOptions(False, False, None)),
  17: Rule(NonTerminal('value_item'), [NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('expr_type_name'), Terminal('SEMICOLON', True)], None, RuleOptions(False, False, None)),
  18: Rule(NonTerminal('program_decl'), [Terminal('PROGRAM', True), NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('type_names'), Terminal('RPAR', True), Terminal('__ANON_0', True), NonTerminal('type_name'), Terminal('SEMICOLON', True)], None, RuleOptions(False, False, None)),
  19: Rule(NonTerminal('func_decls'), [], None, RuleOptions(False, False, None)),
  20: Rule(NonTerminal('func_decls'), [NonTerminal('__anon_star_3')], None, RuleOptions(False, False, None)),
  21: Rule(NonTerminal('func_decl'), [Terminal('FUNC', True), NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('func_body'), NonTerminal('func_constraints')], None, RuleOptions(False, False, None)),
  22: Rule(NonTerminal('func_body'), [NonTerminal('func_lhs'), Terminal('__ANON_0', True), NonTerminal('func_rhss')], None, RuleOptions(False, False, None)),
  23: Rule(NonTerminal('func_lhs'), [NonTerminal('opt_arg')], None, RuleOptions(False, True, None)),
  24: Rule(NonTerminal('func_rhss'), [NonTerminal('func_rhs'), NonTerminal('__anon_star_4')], None, RuleOptions(False, False, None)),
  25: Rule(NonTerminal('func_rhss'), [NonTerminal('func_rhs')], None, RuleOptions(False, False, None)),
  26: Rule(NonTerminal('func_rhs'), [NonTerminal('opt_arg')], None, RuleOptions(False, True, None)),
  27: Rule(NonTerminal('opt_arg'), [NonTerminal('type_name')], None, RuleOptions(False, False, None)),
  28: Rule(NonTerminal('opt_arg'), [NonTerminal('type_name'), NonTerminal('var_name')], None, RuleOptions(False, False, None)),
  29: Rule(NonTerminal('func_constraints'), [Terminal('LBRACE', True), NonTerminal('func_constraint_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  30: Rule(NonTerminal('func_constraints'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  31: Rule(NonTerminal('func_constraint_items'), [NonTerminal('func_constraint_item'), NonTerminal('__anon_star_5')], None, RuleOptions(False, False, None)),
  32: Rule(NonTerminal('func_constraint_items'), [NonTerminal('func_constraint_item')], None, RuleOptions(False, False, None)),
  33: Rule(NonTerminal('func_constraint_item'), [NonTerminal('expr'), Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  34: Rule(NonTerminal('expr'), [NonTerminal('cond_expr')], None, RuleOptions(False, True, None)),
  35: Rule(NonTerminal('expr'), [NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  36: Rule(NonTerminal('imply_expr'), [NonTerminal('or_expr'), Terminal('__ANON_1', True), NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  37: Rule(NonTerminal('imply_expr'), [NonTerminal('or_expr')], None, RuleOptions(False, True, None)),
  38: Rule(NonTerminal('or_expr'), [NonTerminal('and_expr')], None, RuleOptions(False, True, None)),
  39: Rule(NonTerminal('or_expr'), [NonTerminal('or_expr'), Terminal('__ANON_2', True), NonTerminal('and_expr')], None, RuleOptions(False, True, None)),
  40: Rule(NonTerminal('and_expr'), [NonTerminal('cmp_expr')], None, RuleOptions(False, True, None)),
  41: Rule(NonTerminal('and_expr'), [NonTerminal('and_expr'), Terminal('__ANON_3', True), NonTerminal('cmp_expr')], None, RuleOptions(False, True, None)),
  42: Rule(NonTerminal('cmp_expr'), [NonTerminal('cmp_expr'), NonTerminal('cmp_op'), NonTerminal('term_expr')], None, RuleOptions(False, True, None)),
  43: Rule(NonTerminal('cmp_expr'), [NonTerminal('term_expr')], None, RuleOptions(False, True, None)),
  44: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_4', True)], 'expr_eq', RuleOptions(False, True, None)),
  45: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_6', True)], 'expr_le', RuleOptions(False, True, None)),
  46: Rule(NonTerminal('cmp_op'), [Terminal('MORETHAN', True)], 'expr_gt', RuleOptions(False, True, None)),
  47: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_5', True)], 'expr_ne', RuleOptions(False, True, None)),
  48: Rule(NonTerminal('cmp_op'), [Terminal('LESSTHAN', True)], 'expr_lt', RuleOptions(False, True, None)),
  49: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_7', True)], 'expr_ge', RuleOptions(False, True, None)),
  50: Rule(NonTerminal('term_expr'), [NonTerminal('term_expr'), NonTerminal('term_op'), NonTerminal('factor_expr')], None, RuleOptions(False, True, None)),
  51: Rule(NonTerminal('term_expr'), [NonTerminal('factor_expr')], None, RuleOptions(False, True, None)),
  52: Rule(NonTerminal('term_op'), [Terminal('MINUS', True)], 'expr_sub', RuleOptions(False, True, None)),
  53: Rule(NonTerminal('term_op'), [Terminal('PLUS', True)], 'expr_add', RuleOptions(False, True, None)),
  54: Rule(NonTerminal('factor_expr'), [NonTerminal('unary_expr')], None, RuleOptions(False, True, None)),
  55: Rule(NonTerminal('factor_expr'), [NonTerminal('factor_expr'), NonTerminal('factor_op'), NonTerminal('unary_expr')], None, RuleOptions(False, True, None)),
  56: Rule(NonTerminal('factor_op'), [Terminal('STAR', True)], 'expr_mul', RuleOptions(False, True, None)),
  57: Rule(NonTerminal('factor_op'), [Terminal('PERCENT', True)], 'expr_mod', RuleOptions(False, True, None)),
  58: Rule(NonTerminal('factor_op'), [Terminal('SLASH', True)], 'expr_div', RuleOptions(False, True, None)),
  59: Rule(NonTerminal('unary_expr'), [NonTerminal('unary_op'), NonTerminal('atom_expr')], None, RuleOptions(False, True, None)),
  60: Rule(NonTerminal('unary_expr'), [NonTerminal('atom_expr')], None, RuleOptions(False, True, None)),
  61: Rule(NonTerminal('unary_op'), [Terminal('MINUS', True)], 'expr_neg', RuleOptions(False, True, None)),
  62: Rule(NonTerminal('unary_op'), [Terminal('BANG', True)], 'expr_not', RuleOptions(False, True, None)),
  63: Rule(NonTerminal('atom_expr'), [Terminal('LPAR', True), NonTerminal('expr'), Terminal('RPAR', True)], None, RuleOptions(False, True, None)),
  64: Rule(NonTerminal('atom_expr'), [NonTerminal('var_expr')], None, RuleOptions(False, True, None)),
  65: Rule(NonTerminal('atom_expr'), [NonTerminal('const_expr')], None, RuleOptions(False, True, None)),
  66: Rule(NonTerminal('atom_expr'), [NonTerminal('property_expr')], None, RuleOptions(False, True, None)),
  67: Rule(NonTerminal('const_expr'), [Terminal('INT', False)], 'expr_intlit', RuleOptions(False, True, None)),
  68: Rule(NonTerminal('const_expr'), [Terminal('FLOAT', False)], 'expr_floatlit', RuleOptions(False, True, None)),
  69: Rule(NonTerminal('const_expr'), [Terminal('FALSE', True)], 'expr_false', RuleOptions(False, True, None)),
  70: Rule(NonTerminal('const_expr'), [Terminal('TRUE', True)], 'expr_true', RuleOptions(False, True, None)),
  71: Rule(NonTerminal('var_expr'), [NonTerminal('var_name')], 'expr_var', RuleOptions(False, True, None)),
  72: Rule(NonTerminal('cond_expr'), [Terminal('IF', True), NonTerminal('imply_expr'), Terminal('THEN', True), NonTerminal('imply_expr'), Terminal('ELSE', True), NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  73: Rule(NonTerminal('property_expr'), [NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('var_expr'), Terminal('RPAR', True)], None, RuleOptions(False, True, None)),
  74: Rule(NonTerminal('type_names'), [NonTerminal('type_name')], None, RuleOptions(False, False, None)),
  75: Rule(NonTerminal('type_names'), [], None, RuleOptions(False, False, None)),
  76: Rule(NonTerminal('type_names'), [NonTerminal('type_name'), NonTerminal('__anon_star_6')], None, RuleOptions(False, False, None)),
  77: Rule(NonTerminal('global_preds'), [], None, RuleOptions(False, False, None)),
  78: Rule(NonTerminal('global_preds'), [NonTerminal('__anon_star_7')], None, RuleOptions(False, False, None)),
  79: Rule(NonTerminal('global_pred'), [Terminal('PREDICATE', True), NonTerminal('pred_body'), Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  80: Rule(NonTerminal('pred_body'), [NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('pred_args'), Terminal('RPAR', True)], None, RuleOptions(False, False, None)),
  81: Rule(NonTerminal('pred_args'), [], None, RuleOptions(False, False, None)),
  82: Rule(NonTerminal('pred_args'), [NonTerminal('pred_arg'), NonTerminal('__anon_star_8')], None, RuleOptions(False, False, None)),
  83: Rule(NonTerminal('pred_args'), [NonTerminal('pred_arg')], None, RuleOptions(False, False, None)),
  84: Rule(NonTerminal('pred_arg'), [Terminal('TRUE', True)], 'pred_true', RuleOptions(False, False, None)),
  85: Rule(NonTerminal('pred_arg'), [Terminal('FALSE', True)], 'pred_false', RuleOptions(False, False, None)),
  86: Rule(NonTerminal('pred_arg'), [Terminal('STRLIT', False)], 'pred_str', RuleOptions(False, False, None)),
  87: Rule(NonTerminal('pred_arg'), [NonTerminal('var_name')], 'pred_var', RuleOptions(False, False, None)),
  88: Rule(NonTerminal('pred_arg'), [Terminal('SNUMBER', False)], 'pred_num', RuleOptions(False, False, None)),
  89: Rule(NonTerminal('enum_item'), [Terminal('STRLIT', False)], None, RuleOptions(False, True, None)),
  90: Rule(NonTerminal('expr_type_name'), [Terminal('BOOL', True)], 'expr_bool', RuleOptions(False, True, None)),
  91: Rule(NonTerminal('expr_type_name'), [Terminal('__ANON_8', True)], 'expr_int', RuleOptions(False, True, None)),
  92: Rule(NonTerminal('expr_type_name'), [Terminal('REAL', True)], 'expr_real', RuleOptions(False, True, None)),
  93: Rule(NonTerminal('type_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  94: Rule(NonTerminal('var_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  95: Rule(NonTerminal('func_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  96: Rule(NonTerminal('__anon_star_0'), [NonTerminal('__anon_star_0'), NonTerminal('type_decl')], None, None),
  97: Rule(NonTerminal('__anon_star_0'), [NonTerminal('type_decl')], None, None),
  98: Rule(NonTerminal('__anon_star_1'), [Terminal('COMMA', True), NonTerminal('enum_item')], None, None),
  99: Rule(NonTerminal('__anon_star_1'), [NonTerminal('__anon_star_1'), Terminal('COMMA', True), NonTerminal('enum_item')], None, None),
  100: Rule(NonTerminal('__anon_star_2'), [NonTerminal('value_item')], None, None),
  101: Rule(NonTerminal('__anon_star_2'), [NonTerminal('__anon_star_2'), NonTerminal('value_item')], None, None),
  102: Rule(NonTerminal('__anon_star_3'), [NonTerminal('func_decl')], None, None),
  103: Rule(NonTerminal('__anon_star_3'), [NonTerminal('__anon_star_3'), NonTerminal('func_decl')], None, None),
  104: Rule(NonTerminal('__anon_star_4'), [NonTerminal('__anon_star_4'), Terminal('COMMA', True), NonTerminal('func_rhs')], None, None),
  105: Rule(NonTerminal('__anon_star_4'), [Terminal('COMMA', True), NonTerminal('func_rhs')], None, None),
  106: Rule(NonTerminal('__anon_star_5'), [NonTerminal('__anon_star_5'), NonTerminal('func_constraint_item')], None, None),
  107: Rule(NonTerminal('__anon_star_5'), [NonTerminal('func_constraint_item')], None, None),
  108: Rule(NonTerminal('__anon_star_6'), [Terminal('COMMA', True), NonTerminal('type_name')], None, None),
  109: Rule(NonTerminal('__anon_star_6'), [NonTerminal('__anon_star_6'), Terminal('COMMA', True), NonTerminal('type_name')], None, None),
  110: Rule(NonTerminal('__anon_star_7'), [NonTerminal('__anon_star_7'), NonTerminal('global_pred')], None, None),
  111: Rule(NonTerminal('__anon_star_7'), [NonTerminal('global_pred')], None, None),
  112: Rule(NonTerminal('__anon_star_8'), [NonTerminal('__anon_star_8'), Terminal('COMMA', True), NonTerminal('pred_arg')], None, None),
  113: Rule(NonTerminal('__anon_star_8'), [Terminal('COMMA', True), NonTerminal('pred_arg')], None, None),
}
parse_tree_builder = ParseTreeBuilder(RULES.values(), Tree)
class ParseTable: pass
parse_table = ParseTable()
STATES = {
  0: {0: (1, 1), 1: (0, 1), 2: (0, 2), 3: (0, 3), 4: (0, 4), 5: (0, 5), 6: (0, 6), 7: (0, 7), 8: (0, 8), 9: (0, 9), 10: (0, 10)},
  1: {11: (0, 11)},
  2: {0: (1, 97), 4: (1, 97), 8: (1, 97), 6: (1, 97)},
  3: {0: (0, 12), 12: (0, 13)},
  4: {13: (0, 14), 14: (0, 15)},
  5: {0: (1, 5), 4: (1, 5), 6: (1, 5), 8: (1, 5)},
  6: {14: (0, 15), 13: (0, 16)},
  7: {0: (1, 2), 4: (0, 4), 5: (0, 5), 6: (0, 6), 2: (0, 17), 8: (0, 8), 10: (0, 10), 9: (0, 9)},
  8: {13: (0, 18), 14: (0, 15)},
  9: {0: (1, 3), 4: (1, 3), 6: (1, 3), 8: (1, 3)},
  10: {0: (1, 4), 4: (1, 4), 6: (1, 4), 8: (1, 4)},
  11: {},
  12: {14: (0, 19), 15: (0, 20)},
  13: {11: (1, 19), 16: (1, 19), 17: (0, 21), 18: (0, 22), 19: (0, 23), 20: (0, 24)},
  14: {21: (0, 25), 22: (0, 26), 23: (0, 27)},
  15: {24: (1, 93), 22: (1, 93), 25: (1, 93), 14: (1, 93), 26: (1, 93), 23: (1, 93), 27: (1, 93)},
  16: {26: (0, 28)},
  17: {0: (1, 96), 4: (1, 96), 8: (1, 96), 6: (1, 96)},
  18: {23: (0, 29), 28: (0, 30), 22: (0, 31)},
  19: {29: (1, 95), 30: (1, 95)},
  20: {29: (0, 32)},
  21: {11: (1, 77), 31: (0, 33), 32: (0, 34), 33: (0, 35), 16: (0, 36)},
  22: {11: (1, 20), 16: (1, 20), 20: (0, 37), 19: (0, 23)},
  23: {14: (0, 19), 15: (0, 38)},
  24: {11: (1, 102), 19: (1, 102), 16: (1, 102)},
  25: {0: (1, 12), 4: (1, 12), 6: (1, 12), 8: (1, 12)},
  26: {14: (0, 19), 15: (0, 39), 34: (0, 40), 35: (0, 41)},
  27: {0: (1, 14), 4: (1, 14), 6: (1, 14), 8: (1, 14)},
  28: {36: (0, 42)},
  29: {0: (1, 8), 4: (1, 8), 6: (1, 8), 8: (1, 8)},
  30: {0: (1, 6), 4: (1, 6), 6: (1, 6), 8: (1, 6)},
  31: {37: (0, 43), 38: (0, 44), 39: (0, 45)},
  32: {25: (1, 75), 13: (0, 46), 40: (0, 47), 14: (0, 15)},
  33: {11: (1, 78), 16: (0, 36), 32: (0, 48)},
  34: {11: (1, 111), 16: (1, 111)},
  35: {11: (1, 0)},
  36: {14: (0, 19), 15: (0, 49), 41: (0, 50)},
  37: {11: (1, 103), 19: (1, 103), 16: (1, 103)},
  38: {30: (0, 51)},
  39: {30: (0, 52)},
  40: {42: (1, 16), 14: (0, 19), 43: (0, 53), 34: (0, 54), 15: (0, 39)},
  41: {42: (0, 55)},
  42: {44: (0, 56)},
  43: {27: (1, 89), 42: (1, 89)},
  44: {42: (1, 10), 45: (0, 57), 27: (0, 58)},
  45: {42: (0, 59)},
  46: {25: (1, 74), 27: (0, 60), 46: (0, 61)},
  47: {25: (0, 62)},
  48: {11: (1, 110), 16: (1, 110)},
  49: {29: (0, 63)},
  50: {23: (0, 64)},
  51: {47: (0, 65), 13: (0, 66), 14: (0, 15), 48: (0, 67), 49: (0, 68)},
  52: {50: (0, 69), 51: (0, 70), 52: (0, 71), 53: (0, 72)},
  53: {42: (1, 15), 15: (0, 39), 14: (0, 19), 34: (0, 73)},
  54: {42: (1, 100), 14: (1, 100)},
  55: {0: (1, 13), 4: (1, 13), 6: (1, 13), 8: (1, 13)},
  56: {23: (0, 29), 28: (0, 74), 22: (0, 31)},
  57: {42: (1, 11), 27: (0, 75)},
  58: {37: (0, 43), 38: (0, 76)},
  59: {0: (1, 9), 4: (1, 9), 6: (1, 9), 8: (1, 9)},
  60: {14: (0, 15), 13: (0, 77)},
  61: {25: (1, 76), 27: (0, 78)},
  62: {24: (0, 79)},
  63: {25: (1, 81), 14: (0, 80), 54: (0, 81), 55: (0, 82), 56: (0, 83), 37: (0, 84), 57: (0, 85), 58: (0, 86), 59: (0, 87)},
  64: {11: (1, 79), 16: (1, 79)},
  65: {24: (1, 23)},
  66: {24: (1, 27), 22: (1, 27), 23: (1, 27), 27: (1, 27), 58: (0, 88), 14: (0, 80)},
  67: {23: (0, 89), 22: (0, 90), 60: (0, 91)},
  68: {24: (0, 92)},
  69: {23: (1, 90)},
  70: {23: (0, 93)},
  71: {23: (1, 91)},
  72: {23: (1, 92)},
  73: {42: (1, 101), 14: (1, 101)},
  74: {0: (1, 7), 4: (1, 7), 6: (1, 7), 8: (1, 7)},
  75: {37: (0, 43), 38: (0, 94)},
  76: {27: (1, 98), 42: (1, 98)},
  77: {25: (1, 108), 27: (1, 108)},
  78: {14: (0, 15), 13: (0, 95)},
  79: {14: (0, 15), 13: (0, 96)},
  80: {24: (1, 94), 61: (1, 94), 62: (1, 94), 22: (1, 94), 63: (1, 94), 64: (1, 94), 65: (1, 94), 66: (1, 94), 25: (1, 94), 67: (1, 94), 68: (1, 94), 69: (1, 94), 70: (1, 94), 27: (1, 94), 71: (1, 94), 72: (1, 94), 73: (1, 94), 23: (1, 94), 74: (1, 94), 75: (1, 94), 76: (1, 94)},
  81: {25: (0, 97)},
  82: {25: (1, 84), 27: (1, 84)},
  83: {25: (1, 85), 27: (1, 85)},
  84: {25: (1, 86), 27: (1, 86)},
  85: {25: (1, 88), 27: (1, 88)},
  86: {25: (1, 87), 27: (1, 87)},
  87: {25: (1, 83), 77: (0, 98), 27: (0, 99)},
  88: {24: (1, 28), 22: (1, 28), 23: (1, 28), 27: (1, 28)},
  89: {11: (1, 30), 19: (1, 30), 16: (1, 30)},
  90: {78: (0, 100), 79: (0, 101), 80: (0, 102), 15: (0, 103), 81: (0, 104), 82: (0, 105), 83: (0, 106), 14: (0, 107), 84: (0, 108), 85: (0, 109), 86: (0, 110), 87: (0, 111), 36: (0, 112), 29: (0, 113), 88: (0, 114), 89: (0, 115), 72: (0, 116), 56: (0, 117), 90: (0, 118), 58: (0, 119), 91: (0, 120), 92: (0, 121), 93: (0, 122), 94: (0, 123), 95: (0, 124), 55: (0, 125), 96: (0, 126)},
  91: {11: (1, 21), 19: (1, 21), 16: (1, 21)},
  92: {47: (0, 127), 13: (0, 66), 97: (0, 128), 98: (0, 129), 14: (0, 15)},
  93: {42: (1, 17), 14: (1, 17)},
  94: {27: (1, 99), 42: (1, 99)},
  95: {25: (1, 109), 27: (1, 109)},
  96: {23: (0, 130)},
  97: {23: (1, 80)},
  98: {25: (1, 82), 27: (0, 131)},
  99: {14: (0, 80), 59: (0, 132), 58: (0, 86), 55: (0, 82), 56: (0, 83), 57: (0, 85), 37: (0, 84)},
  100: {25: (1, 34), 23: (1, 34)},
  101: {61: (1, 66), 62: (1, 66), 66: (1, 66), 63: (1, 66), 25: (1, 66), 67: (1, 66), 68: (1, 66), 69: (1, 66), 70: (1, 66), 64: (1, 66), 71: (1, 66), 72: (1, 66), 73: (1, 66), 65: (1, 66), 23: (1, 66), 74: (1, 66), 75: (1, 66), 76: (1, 66)},
  102: {61: (1, 43), 62: (1, 43), 66: (1, 43), 63: (1, 43), 25: (1, 43), 68: (1, 43), 69: (1, 43), 70: (1, 43), 73: (1, 43), 65: (1, 43), 23: (1, 43), 74: (1, 43), 75: (1, 43), 99: (0, 133), 72: (0, 134), 76: (0, 135)},
  103: {29: (0, 136)},
  104: {63: (1, 40), 25: (1, 40), 68: (1, 40), 65: (1, 40), 23: (1, 40), 69: (1, 40), 75: (1, 40), 66: (0, 137), 74: (0, 138), 70: (0, 139), 73: (0, 140), 100: (0, 141), 61: (0, 142), 62: (0, 143)},
  105: {61: (1, 65), 62: (1, 65), 66: (1, 65), 63: (1, 65), 25: (1, 65), 67: (1, 65), 68: (1, 65), 69: (1, 65), 70: (1, 65), 64: (1, 65), 71: (1, 65), 72: (1, 65), 73: (1, 65), 65: (1, 65), 23: (1, 65), 74: (1, 65), 75: (1, 65), 76: (1, 65)},
  106: {23: (1, 37), 25: (1, 37), 75: (1, 37), 68: (1, 37), 65: (0, 144), 63: (0, 145)},
  107: {29: (1, 95), 30: (1, 95), 24: (1, 94), 61: (1, 94), 62: (1, 94), 22: (1, 94), 63: (1, 94), 64: (1, 94), 65: (1, 94), 66: (1, 94), 25: (1, 94), 67: (1, 94), 68: (1, 94), 69: (1, 94), 70: (1, 94), 27: (1, 94), 71: (1, 94), 72: (1, 94), 73: (1, 94), 23: (1, 94), 74: (1, 94), 75: (1, 94), 76: (1, 94)},
  108: {61: (1, 51), 62: (1, 51), 66: (1, 51), 63: (1, 51), 25: (1, 51), 68: (1, 51), 69: (1, 51), 70: (1, 51), 72: (1, 51), 73: (1, 51), 65: (1, 51), 23: (1, 51), 74: (1, 51), 75: (1, 51), 76: (1, 51), 67: (0, 146), 71: (0, 147), 101: (0, 148), 64: (0, 149)},
  109: {36: (1, 62), 29: (1, 62), 96: (1, 62), 14: (1, 62), 55: (1, 62), 56: (1, 62)},
  110: {25: (1, 35), 23: (1, 35)},
  111: {42: (0, 150)},
  112: {61: (1, 67), 62: (1, 67), 63: (1, 67), 64: (1, 67), 65: (1, 67), 66: (1, 67), 25: (1, 67), 67: (1, 67), 68: (1, 67), 69: (1, 67), 70: (1, 67), 71: (1, 67), 72: (1, 67), 73: (1, 67), 23: (1, 67), 74: (1, 67), 75: (1, 67), 76: (1, 67)},
  113: {78: (0, 100), 79: (0, 101), 80: (0, 102), 15: (0, 103), 81: (0, 104), 82: (0, 105), 83: (0, 106), 14: (0, 107), 84: (0, 108), 85: (0, 109), 86: (0, 110), 36: (0, 112), 29: (0, 113), 88: (0, 114), 89: (0, 115), 72: (0, 116), 56: (0, 117), 90: (0, 118), 58: (0, 119), 93: (0, 122), 94: (0, 123), 91: (0, 151), 95: (0, 124), 55: (0, 125), 96: (0, 126)},
  114: {14: (0, 107), 95: (0, 152), 96: (0, 126), 56: (0, 117), 79: (0, 101), 58: (0, 119), 93: (0, 122), 15: (0, 103), 82: (0, 105), 36: (0, 112), 29: (0, 113), 55: (0, 125)},
  115: {83: (0, 106), 72: (0, 116), 56: (0, 117), 79: (0, 101), 90: (0, 118), 80: (0, 102), 15: (0, 103), 58: (0, 119), 93: (0, 122), 94: (0, 123), 82: (0, 105), 81: (0, 104), 95: (0, 124), 55: (0, 125), 84: (0, 108), 14: (0, 107), 86: (0, 153), 96: (0, 126), 88: (0, 114), 85: (0, 109), 36: (0, 112), 29: (0, 113)},
  116: {36: (1, 61), 29: (1, 61), 96: (1, 61), 14: (1, 61), 55: (1, 61), 56: (1, 61)},
  117: {61: (1, 69), 62: (1, 69), 63: (1, 69), 64: (1, 69), 65: (1, 69), 66: (1, 69), 25: (1, 69), 67: (1, 69), 68: (1, 69), 69: (1, 69), 70: (1, 69), 71: (1, 69), 72: (1, 69), 73: (1, 69), 23: (1, 69), 74: (1, 69), 75: (1, 69), 76: (1, 69)},
  118: {61: (1, 54), 62: (1, 54), 66: (1, 54), 63: (1, 54), 25: (1, 54), 67: (1, 54), 68: (1, 54), 69: (1, 54), 70: (1, 54), 64: (1, 54), 71: (1, 54), 72: (1, 54), 73: (1, 54), 65: (1, 54), 23: (1, 54), 74: (1, 54), 75: (1, 54), 76: (1, 54)},
  119: {61: (1, 71), 62: (1, 71), 63: (1, 71), 64: (1, 71), 65: (1, 71), 66: (1, 71), 25: (1, 71), 67: (1, 71), 68: (1, 71), 69: (1, 71), 70: (1, 71), 71: (1, 71), 72: (1, 71), 73: (1, 71), 23: (1, 71), 74: (1, 71), 75: (1, 71), 76: (1, 71)},
  120: {23: (0, 154)},
  121: {42: (1, 32), 78: (0, 100), 79: (0, 101), 80: (0, 102), 102: (0, 155), 15: (0, 103), 81: (0, 104), 82: (0, 105), 83: (0, 106), 14: (0, 107), 84: (0, 108), 85: (0, 109), 86: (0, 110), 36: (0, 112), 29: (0, 113), 88: (0, 114), 89: (0, 115), 72: (0, 116), 56: (0, 117), 90: (0, 118), 58: (0, 119), 91: (0, 120), 93: (0, 122), 94: (0, 123), 95: (0, 124), 55: (0, 125), 96: (0, 126), 92: (0, 156)},
  122: {61: (1, 64), 62: (1, 64), 66: (1, 64), 63: (1, 64), 25: (1, 64), 67: (1, 64), 68: (1, 64), 69: (1, 64), 70: (1, 64), 64: (1, 64), 71: (1, 64), 72: (1, 64), 73: (1, 64), 65: (1, 64), 23: (1, 64), 74: (1, 64), 75: (1, 64), 76: (1, 64)},
  123: {63: (1, 38), 25: (1, 38), 68: (1, 38), 65: (1, 38), 23: (1, 38), 75: (1, 38), 69: (0, 157)},
  124: {61: (1, 60), 62: (1, 60), 66: (1, 60), 63: (1, 60), 25: (1, 60), 67: (1, 60), 68: (1, 60), 69: (1, 60), 70: (1, 60), 64: (1, 60), 71: (1, 60), 72: (1, 60), 73: (1, 60), 65: (1, 60), 23: (1, 60), 74: (1, 60), 75: (1, 60), 76: (1, 60)},
  125: {61: (1, 70), 62: (1, 70), 63: (1, 70), 64: (1, 70), 65: (1, 70), 66: (1, 70), 25: (1, 70), 67: (1, 70), 68: (1, 70), 69: (1, 70), 70: (1, 70), 71: (1, 70), 72: (1, 70), 73: (1, 70), 23: (1, 70), 74: (1, 70), 75: (1, 70), 76: (1, 70)},
  126: {61: (1, 68), 62: (1, 68), 63: (1, 68), 64: (1, 68), 65: (1, 68), 66: (1, 68), 25: (1, 68), 67: (1, 68), 68: (1, 68), 69: (1, 68), 70: (1, 68), 71: (1, 68), 72: (1, 68), 73: (1, 68), 23: (1, 68), 74: (1, 68), 75: (1, 68), 76: (1, 68)},
  127: {22: (1, 26), 23: (1, 26), 27: (1, 26)},
  128: {22: (1, 22), 23: (1, 22)},
  129: {22: (1, 25), 23: (1, 25), 103: (0, 158), 27: (0, 159)},
  130: {11: (1, 18), 16: (1, 18), 19: (1, 18)},
  131: {14: (0, 80), 58: (0, 86), 55: (0, 82), 56: (0, 83), 59: (0, 160), 57: (0, 85), 37: (0, 84)},
  132: {25: (1, 113), 27: (1, 113)},
  133: {72: (0, 116), 56: (0, 117), 84: (0, 161), 79: (0, 101), 90: (0, 118), 58: (0, 119), 93: (0, 122), 15: (0, 103), 82: (0, 105), 95: (0, 124), 55: (0, 125), 14: (0, 107), 96: (0, 126), 85: (0, 109), 36: (0, 112), 29: (0, 113), 88: (0, 114)},
  134: {85: (1, 52), 36: (1, 52), 29: (1, 52), 72: (1, 52), 96: (1, 52), 55: (1, 52), 14: (1, 52), 56: (1, 52)},
  135: {85: (1, 53), 36: (1, 53), 29: (1, 53), 72: (1, 53), 96: (1, 53), 55: (1, 53), 14: (1, 53), 56: (1, 53)},
  136: {93: (0, 162), 58: (0, 119), 14: (0, 80)},
  137: {85: (1, 46), 36: (1, 46), 29: (1, 46), 72: (1, 46), 96: (1, 46), 55: (1, 46), 14: (1, 46), 56: (1, 46)},
  138: {85: (1, 44), 36: (1, 44), 29: (1, 44), 72: (1, 44), 96: (1, 44), 55: (1, 44), 14: (1, 44), 56: (1, 44)},
  139: {85: (1, 48), 36: (1, 48), 29: (1, 48), 72: (1, 48), 96: (1, 48), 55: (1, 48), 14: (1, 48), 56: (1, 48)},
  140: {85: (1, 47), 36: (1, 47), 29: (1, 47), 72: (1, 47), 96: (1, 47), 55: (1, 47), 14: (1, 47), 56: (1, 47)},
  141: {72: (0, 116), 56: (0, 117), 79: (0, 101), 90: (0, 118), 80: (0, 163), 58: (0, 119), 15: (0, 103), 93: (0, 122), 82: (0, 105), 95: (0, 124), 55: (0, 125), 84: (0, 108), 14: (0, 107), 96: (0, 126), 85: (0, 109), 36: (0, 112), 29: (0, 113), 88: (0, 114)},
  142: {85: (1, 49), 36: (1, 49), 29: (1, 49), 72: (1, 49), 96: (1, 49), 55: (1, 49), 14: (1, 49), 56: (1, 49)},
  143: {85: (1, 45), 36: (1, 45), 29: (1, 45), 72: (1, 45), 96: (1, 45), 55: (1, 45), 14: (1, 45), 56: (1, 45)},
  144: {83: (0, 106), 72: (0, 116), 56: (0, 117), 79: (0, 101), 90: (0, 118), 80: (0, 102), 15: (0, 103), 58: (0, 119), 93: (0, 122), 94: (0, 123), 82: (0, 105), 81: (0, 104), 95: (0, 124), 86: (0, 164), 55: (0, 125), 84: (0, 108), 14: (0, 107), 96: (0, 126), 88: (0, 114), 85: (0, 109), 36: (0, 112), 29: (0, 113)},
  145: {72: (0, 116), 56: (0, 117), 94: (0, 165), 79: (0, 101), 90: (0, 118), 80: (0, 102), 15: (0, 103), 58: (0, 119), 93: (0, 122), 82: (0, 105), 81: (0, 104), 95: (0, 124), 55: (0, 125), 84: (0, 108), 14: (0, 107), 96: (0, 126), 85: (0, 109), 36: (0, 112), 29: (0, 113), 88: (0, 114)},
  146: {85: (1, 58), 36: (1, 58), 29: (1, 58), 72: (1, 58), 96: (1, 58), 55: (1, 58), 14: (1, 58), 56: (1, 58)},
  147: {85: (1, 57), 36: (1, 57), 29: (1, 57), 72: (1, 57), 96: (1, 57), 55: (1, 57), 14: (1, 57), 56: (1, 57)},
  148: {72: (0, 116), 56: (0, 117), 79: (0, 101), 58: (0, 119), 93: (0, 122), 15: (0, 103), 90: (0, 166), 82: (0, 105), 95: (0, 124), 55: (0, 125), 14: (0, 107), 96: (0, 126), 85: (0, 109), 36: (0, 112), 29: (0, 113), 88: (0, 114)},
  149: {85: (1, 56), 36: (1, 56), 29: (1, 56), 72: (1, 56), 96: (1, 56), 55: (1, 56), 14: (1, 56), 56: (1, 56)},
  150: {11: (1, 29), 19: (1, 29), 16: (1, 29)},
  151: {25: (0, 167)},
  152: {61: (1, 59), 62: (1, 59), 66: (1, 59), 63: (1, 59), 25: (1, 59), 67: (1, 59), 68: (1, 59), 69: (1, 59), 70: (1, 59), 64: (1, 59), 71: (1, 59), 72: (1, 59), 73: (1, 59), 65: (1, 59), 23: (1, 59), 74: (1, 59), 75: (1, 59), 76: (1, 59)},
  153: {68: (0, 168)},
  154: {36: (1, 33), 29: (1, 33), 55: (1, 33), 89: (1, 33), 85: (1, 33), 72: (1, 33), 96: (1, 33), 42: (1, 33), 14: (1, 33), 56: (1, 33)},
  155: {42: (1, 31), 78: (0, 100), 79: (0, 101), 80: (0, 102), 15: (0, 103), 81: (0, 104), 82: (0, 105), 83: (0, 106), 14: (0, 107), 84: (0, 108), 85: (0, 109), 86: (0, 110), 36: (0, 112), 29: (0, 113), 88: (0, 114), 89: (0, 115), 72: (0, 116), 56: (0, 117), 90: (0, 118), 58: (0, 119), 91: (0, 120), 93: (0, 122), 94: (0, 123), 95: (0, 124), 55: (0, 125), 96: (0, 126), 92: (0, 169)},
  156: {36: (1, 107), 29: (1, 107), 55: (1, 107), 89: (1, 107), 85: (1, 107), 72: (1, 107), 96: (1, 107), 42: (1, 107), 14: (1, 107), 56: (1, 107)},
  157: {72: (0, 116), 56: (0, 117), 79: (0, 101), 90: (0, 118), 80: (0, 102), 81: (0, 170), 15: (0, 103), 58: (0, 119), 93: (0, 122), 82: (0, 105), 95: (0, 124), 55: (0, 125), 84: (0, 108), 14: (0, 107), 96: (0, 126), 85: (0, 109), 36: (0, 112), 29: (0, 113), 88: (0, 114)},
  158: {22: (1, 24), 23: (1, 24), 27: (0, 171)},
  159: {47: (0, 127), 13: (0, 66), 98: (0, 172), 14: (0, 15)},
  160: {25: (1, 112), 27: (1, 112)},
  161: {61: (1, 50), 62: (1, 50), 66: (1, 50), 63: (1, 50), 25: (1, 50), 68: (1, 50), 69: (1, 50), 70: (1, 50), 72: (1, 50), 73: (1, 50), 65: (1, 50), 23: (1, 50), 74: (1, 50), 75: (1, 50), 76: (1, 50), 67: (0, 146), 71: (0, 147), 101: (0, 148), 64: (0, 149)},
  162: {25: (0, 173)},
  163: {61: (1, 42), 62: (1, 42), 66: (1, 42), 63: (1, 42), 25: (1, 42), 68: (1, 42), 69: (1, 42), 70: (1, 42), 73: (1, 42), 65: (1, 42), 23: (1, 42), 74: (1, 42), 75: (1, 42), 99: (0, 133), 72: (0, 134), 76: (0, 135)},
  164: {23: (1, 36), 25: (1, 36), 75: (1, 36), 68: (1, 36)},
  165: {63: (1, 39), 25: (1, 39), 68: (1, 39), 65: (1, 39), 23: (1, 39), 75: (1, 39), 69: (0, 157)},
  166: {61: (1, 55), 62: (1, 55), 66: (1, 55), 63: (1, 55), 25: (1, 55), 67: (1, 55), 68: (1, 55), 69: (1, 55), 70: (1, 55), 64: (1, 55), 71: (1, 55), 72: (1, 55), 73: (1, 55), 65: (1, 55), 23: (1, 55), 74: (1, 55), 75: (1, 55), 76: (1, 55)},
  167: {61: (1, 63), 62: (1, 63), 66: (1, 63), 63: (1, 63), 25: (1, 63), 67: (1, 63), 68: (1, 63), 69: (1, 63), 70: (1, 63), 64: (1, 63), 71: (1, 63), 72: (1, 63), 73: (1, 63), 65: (1, 63), 23: (1, 63), 74: (1, 63), 75: (1, 63), 76: (1, 63)},
  168: {83: (0, 106), 72: (0, 116), 56: (0, 117), 79: (0, 101), 90: (0, 118), 80: (0, 102), 15: (0, 103), 58: (0, 119), 93: (0, 122), 94: (0, 123), 82: (0, 105), 81: (0, 104), 95: (0, 124), 86: (0, 174), 55: (0, 125), 84: (0, 108), 14: (0, 107), 96: (0, 126), 88: (0, 114), 85: (0, 109), 36: (0, 112), 29: (0, 113)},
  169: {36: (1, 106), 29: (1, 106), 55: (1, 106), 89: (1, 106), 85: (1, 106), 72: (1, 106), 96: (1, 106), 42: (1, 106), 14: (1, 106), 56: (1, 106)},
  170: {63: (1, 41), 25: (1, 41), 68: (1, 41), 65: (1, 41), 23: (1, 41), 69: (1, 41), 75: (1, 41), 66: (0, 137), 74: (0, 138), 70: (0, 139), 73: (0, 140), 100: (0, 141), 61: (0, 142), 62: (0, 143)},
  171: {47: (0, 127), 98: (0, 175), 13: (0, 66), 14: (0, 15)},
  172: {22: (1, 105), 23: (1, 105), 27: (1, 105)},
  173: {61: (1, 73), 62: (1, 73), 63: (1, 73), 64: (1, 73), 65: (1, 73), 66: (1, 73), 25: (1, 73), 67: (1, 73), 68: (1, 73), 69: (1, 73), 70: (1, 73), 71: (1, 73), 72: (1, 73), 73: (1, 73), 23: (1, 73), 74: (1, 73), 75: (1, 73), 76: (1, 73)},
  174: {75: (0, 176)},
  175: {22: (1, 104), 23: (1, 104), 27: (1, 104)},
  176: {83: (0, 106), 72: (0, 116), 56: (0, 117), 79: (0, 101), 90: (0, 118), 80: (0, 102), 15: (0, 103), 58: (0, 119), 93: (0, 122), 86: (0, 177), 94: (0, 123), 82: (0, 105), 81: (0, 104), 95: (0, 124), 55: (0, 125), 84: (0, 108), 14: (0, 107), 96: (0, 126), 88: (0, 114), 85: (0, 109), 36: (0, 112), 29: (0, 113)},
  177: {25: (1, 72), 23: (1, 72)},
}
TOKEN_TYPES = (
{0: 'PROGRAM',
 1: 'start',
 2: 'type_decl',
 3: 'type_decls',
 4: 'VALUE',
 5: 'enum_decl',
 6: 'ENUMSET',
 7: '__anon_star_0',
 8: 'ENUM',
 9: 'enum_set_decl',
 10: 'value_decl',
 11: '$END',
 12: 'program_decl',
 13: 'type_name',
 14: 'NAME',
 15: 'func_name',
 16: 'PREDICATE',
 17: 'func_decls',
 18: '__anon_star_3',
 19: 'FUNC',
 20: 'func_decl',
 21: 'value_body',
 22: 'LBRACE',
 23: 'SEMICOLON',
 24: '__ANON_0',
 25: 'RPAR',
 26: 'LSQB',
 27: 'COMMA',
 28: 'enum_body',
 29: 'LPAR',
 30: 'COLON',
 31: '__anon_star_7',
 32: 'global_pred',
 33: 'global_preds',
 34: 'value_item',
 35: 'value_items',
 36: 'INT',
 37: 'STRLIT',
 38: 'enum_item',
 39: 'enum_items',
 40: 'type_names',
 41: 'pred_body',
 42: 'RBRACE',
 43: '__anon_star_2',
 44: 'RSQB',
 45: '__anon_star_1',
 46: '__anon_star_6',
 47: 'opt_arg',
 48: 'func_body',
 49: 'func_lhs',
 50: 'BOOL',
 51: 'expr_type_name',
 52: '__ANON_8',
 53: 'REAL',
 54: 'pred_args',
 55: 'TRUE',
 56: 'FALSE',
 57: 'SNUMBER',
 58: 'var_name',
 59: 'pred_arg',
 60: 'func_constraints',
 61: '__ANON_7',
 62: '__ANON_6',
 63: '__ANON_2',
 64: 'STAR',
 65: '__ANON_1',
 66: 'MORETHAN',
 67: 'SLASH',
 68: 'THEN',
 69: '__ANON_3',
 70: 'LESSTHAN',
 71: 'PERCENT',
 72: 'MINUS',
 73: '__ANON_5',
 74: '__ANON_4',
 75: 'ELSE',
 76: 'PLUS',
 77: '__anon_star_8',
 78: 'cond_expr',
 79: 'property_expr',
 80: 'term_expr',
 81: 'cmp_expr',
 82: 'const_expr',
 83: 'or_expr',
 84: 'factor_expr',
 85: 'BANG',
 86: 'imply_expr',
 87: 'func_constraint_items',
 88: 'unary_op',
 89: 'IF',
 90: 'unary_expr',
 91: 'expr',
 92: 'func_constraint_item',
 93: 'var_expr',
 94: 'and_expr',
 95: 'atom_expr',
 96: 'FLOAT',
 97: 'func_rhss',
 98: 'func_rhs',
 99: 'term_op',
 100: 'cmp_op',
 101: 'factor_op',
 102: '__anon_star_5',
 103: '__anon_star_4'}
)
parse_table.states = {s: {TOKEN_TYPES[t]: (a, RULES[x] if a is Reduce else x) for t, (a, x) in acts.items()}
                      for s, acts in STATES.items()}
parse_table.start_state = 0
parse_table.end_state = 11
class Lark_StandAlone:
  def __init__(self, transformer=None, postlex=None):
     callback = parse_tree_builder.create_callback(transformer=transformer)
     callbacks = {rule: getattr(callback, rule.alias or rule.origin, None) for rule in RULES.values()}
     self.parser = _Parser(parse_table, callbacks)
     self.postlex = postlex
  def parse(self, stream):
     tokens = lex(stream)
     sps = CON_LEXER.set_parser_state
     if self.postlex: tokens = self.postlex.process(tokens)
     return self.parser.parse(tokens, sps)
